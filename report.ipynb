{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Screening App - Report {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Components: {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app.py : containing the script to be run for the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "utils.py : contains the working of the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. utils.py : {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extracting data from a single pdf :\n",
    "\n",
    "- Using pypdf to extract data, page by page into a single 'text' object and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_doc):\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf_doc)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Extracting data from all uploaded pdf files : {-}\n",
    "\n",
    "- The user will uploads multiple pdfs, to process them, the create_docs function works as: \n",
    "\n",
    "1. Recieves the list of uploaded documents and unique ids\n",
    "2. Instantiate a list <b> docs </b> to store all information\n",
    "2. Iterate over all document\n",
    "3. For each document:\n",
    "> 3.1 apply get_pdf_text function to extract all text data from the pdf <br>\n",
    "> 3.2 Create a Langchain schema <b>(Document)</b> taking data from result fo 3.1 and having <u> metadata</u> fields : name,type,size,unique_id populated from the document <br>\n",
    "> 3.3 Append the Document schema object to the <b>Docs</b> list <br><br>\n",
    "\n",
    "4. Return the data of all uploaded pdfs stored in Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(user_pdf_list, unique_id):\n",
    "    docs=[]\n",
    "    for filename in user_pdf_list:\n",
    "        \n",
    "        chunks=get_pdf_text(filename)\n",
    "\n",
    "        #Adding items to our list - Adding data & its metadata\n",
    "        docs.append(Document(\n",
    "            page_content=chunks,\n",
    "        metadata={\"name\": filename.name,\"id\":filename.file_id,\"type=\":filename.type,\"size\":filename.size,\"unique_id\":unique_id},\n",
    "        ))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating embeddings model object : {-}\n",
    "\n",
    "- The downloaded model will make embeddings (i.e.) vectors from given text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_load_data():\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generating vectors of data and uploading to Pinecone : {-}\n",
    "\n",
    "- Applying the embeddings model to the data \n",
    "- Generating the embedding vectors of the data\n",
    "- Storing the embedding vectors to the given index in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_pinecone(pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings,docs):\n",
    "\n",
    "    pinecone.init(\n",
    "    api_key=pinecone_apikey,\n",
    "    environment=pinecone_environment\n",
    "    )\n",
    "    \n",
    "    Pinecone.from_documents(docs, embeddings, index_name=pinecone_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Downloading the vectors from Pinecone : {-}\n",
    "\n",
    "- In <b>2.4</b>, we uploaded the generated vectors to an index in Pinecone.\n",
    "- Here, we are downloading all the vectors from that index using the <b> .from_existing_index() </b> function\n",
    "- Returning the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_from_pinecone(pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings):\n",
    "    print(\"20secs delay...\")\n",
    "    time.sleep(20)\n",
    "    pinecone.init(\n",
    "    api_key=pinecone_apikey,\n",
    "    environment=pinecone_environment\n",
    "    )\n",
    "\n",
    "    index_name = pinecone_index_name\n",
    "\n",
    "    index = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Extracting relevant documents from Pinecone : {-}\n",
    "\n",
    "- The idea behind selecting resume is to find the resume documents that match the JD.\n",
    "- The relevant documents will be the ones having similarity to the text of the JD.\n",
    "- <u> Using pull_from_pinecone() defined in 2.5</u> , we download all documents stored in the Pinecone index,\n",
    "- The .similarity_search_with_score() function is used to find the similarity of the JD to all of the documents.\n",
    "- Return the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_docs(query,k,pinecone_apikey,pinecone_environment,pinecone_index_name,embeddings,unique_id):\n",
    "\n",
    "    pinecone.init(\n",
    "    api_key=pinecone_apikey,\n",
    "    environment=pinecone_environment\n",
    "    )\n",
    "\n",
    "    index_name = pinecone_index_name\n",
    "\n",
    "    index = pull_from_pinecone(pinecone_apikey,pinecone_environment,index_name,embeddings)\n",
    "    similar_docs = index.similarity_search_with_score(query, int(k),{\"unique_id\":unique_id})\n",
    "    #print(similar_docs)\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Summarise the contents of a document : {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(current_doc):\n",
    "    llm = OpenAI(temperature=0)\n",
    "    #llm = HuggingFaceHub(repo_id=\"bigscience/bloom\", model_kwargs={\"temperature\":1e-10})\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    summary = chain.run([current_doc])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
